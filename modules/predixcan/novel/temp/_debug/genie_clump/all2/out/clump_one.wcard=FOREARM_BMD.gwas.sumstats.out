Sender: LSF System <lsfadmin@node28-4>
Subject: Job 110455346: </sc/orga/projects/va-biobank/Veera/genie/modules/predixcan/novel/temp/.snakemake/tmp.3u4xkz3n/all2.clump_one.116> in cluster <minerva> Exited

Job </sc/orga/projects/va-biobank/Veera/genie/modules/predixcan/novel/temp/.snakemake/tmp.3u4xkz3n/all2.clump_one.116> was submitted from host <node26-55> by user <xrajagv01> in cluster <minerva>.
Job was executed on host(s) <node28-4>, in queue <premium>, as user <xrajagv01> in cluster <minerva>.
</hpc/users/xrajagv01> was used as the home directory.
</hpc/users/xrajagv01/va-biobank/Veera/genie/modules/predixcan/novel/temp> was used as the working directory.
Started at Thu Sep  6 05:41:47 2018
Results reported on Thu Sep  6 05:42:32 2018

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/sc/orga/projects/va-biobank/Veera/genie/modules/predixcan/novel/temp/.snakemake/tmp.3u4xkz3n/all2.clump_one.116
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   26.38 sec.
    Max Memory :                                 486 MB
    Average Memory :                             232.71 MB
    Total Requested Memory :                     2000.00 MB
    Delta Memory :                               1514.00 MB
    Max Processes :                              7
    Max Threads :                                9

The output (if any) follows:

[Thu Sep  6 05:41:50 2018] Building DAG of jobs...
[Thu Sep  6 05:42:02 2018] Using shell: /bin/bash
[Thu Sep  6 05:42:02 2018] Provided cores: 1
[Thu Sep  6 05:42:02 2018] Rules claiming more threads will be scaled down.
[Thu Sep  6 05:42:02 2018] Job counts:
[Thu Sep  6 05:42:02 2018] 	count	jobs
[Thu Sep  6 05:42:02 2018] 	1	clump_one
[Thu Sep  6 05:42:02 2018] 	1

[Thu Sep  6 05:42:02 2018] rule clump_one:
[Thu Sep  6 05:42:02 2018]     input: 1kg.eur.biallele.snps.maf0.01.bed, 1kg.eur.biallele.snps.maf0.01.bim, 1kg.eur.biallele.snps.maf0.01.fam, /sc/orga/projects/va-biobank/Veera/downloads/CMC/modules/wen_gwas_files/FOREARM_BMD.gwas.sumstats
[Thu Sep  6 05:42:02 2018]     output: genie_clump/all2/all2ld_from_1kg.eur.biallele.snps.maf0.01/FOREARM_BMD.gwas.sumstats.clumped
[Thu Sep  6 05:42:02 2018]     jobid: 0
[Thu Sep  6 05:42:02 2018]     wildcards: wcard=FOREARM_BMD.gwas.sumstats

PLINK v1.90b3.35 64-bit (25 Mar 2016)      https://www.cog-genomics.org/plink2
(C) 2005-2016 Shaun Purcell, Christopher Chang   GNU General Public License v3
Logging to genie_clump/all2/all2ld_from_1kg.eur.biallele.snps.maf0.01/FOREARM_BMD.gwas.sumstats.log.
Options in effect:
  --bfile 1kg.eur.biallele.snps.maf0.01
  --clump /sc/orga/projects/va-biobank/Veera/downloads/CMC/modules/wen_gwas_files/FOREARM_BMD.gwas.sumstats
  --clump-kb 250
  --clump-p1 0.00000005
  --clump-p2 0.00000005
  --clump-r2 0.1
  --out genie_clump/all2/all2ld_from_1kg.eur.biallele.snps.maf0.01/FOREARM_BMD.gwas.sumstats

64316 MB RAM detected; reserving 32158 MB for main workspace.
8783349 variants loaded from .bim file.
503 people (0 males, 0 females, 503 ambiguous) loaded from .fam.
Ambiguous sex IDs written to
genie_clump/all2/all2ld_from_1kg.eur.biallele.snps.maf0.01/FOREARM_BMD.gwas.sumstats.nosex
.
Using 1 thread (no multithreaded calculations invoked).
Before main variant filters, 503 founders and 0 nonfounders present.
Calculating allele frequencies... 0%1%2%3%4%5%6%7%8%9%10%11%12%13%14%15%16%17%18%19%20%21%22%23%24%25%26%27%28%29%30%31%32%33%34%35%36%37%38%39%40%41%42%43%44%45%46%47%48%49%50%51%52%53%54%55%56%57%58%59%60%61%62%63%64%65%66%67%68%69%70%71%72%73%74%75%76%77%78%79%80%81%82%83%84%85%86%87%88%89%90%91%92%93%94%95%96%97%98%99% done.
8783349 variants and 503 people pass filters and QC.
Note: No phenotypes present.

Error: File read failure.
[Thu Sep  6 05:42:31 2018]     Error in rule clump_one:
[Thu Sep  6 05:42:31 2018]         jobid: 0
[Thu Sep  6 05:42:31 2018]         output: genie_clump/all2/all2ld_from_1kg.eur.biallele.snps.maf0.01/FOREARM_BMD.gwas.sumstats.clumped

[Thu Sep  6 05:42:31 2018] RuleException:
[Thu Sep  6 05:42:31 2018] CalledProcessError in line 38 of /sc/orga/projects/va-biobank/Veera/genie/modules/sumstats/clump/clump.snake:
[Thu Sep  6 05:42:31 2018] Command ' set -euo pipefail;  module load plink/1.90 &&  plink --bfile 1kg.eur.biallele.snps.maf0.01 --clump /sc/orga/projects/va-biobank/Veera/downloads/CMC/modules/wen_gwas_files/FOREARM_BMD.gwas.sumstats --out genie_clump/all2/all2ld_from_1kg.eur.biallele.snps.maf0.01/FOREARM_BMD.gwas.sumstats --clump-p1 0.00000005 --clump-p2 0.00000005 --clump-r2 0.1 --clump-kb 250 ' returned non-zero exit status 7.
[Thu Sep  6 05:42:31 2018]   File "/sc/orga/projects/va-biobank/Veera/genie/modules/sumstats/clump/clump.snake", line 38, in __rule_clump_one
[Thu Sep  6 05:42:31 2018]   File "/hpc/users/xrajagv01/.conda/envs/genie/lib/python3.6/concurrent/futures/thread.py", line 56, in run
[Thu Sep  6 05:42:31 2018] Shutting down, this might take some time.
[Thu Sep  6 05:42:31 2018] Exiting because a job execution failed. Look above for error message
