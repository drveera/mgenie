Sender: LSF System <lsfadmin@node26-50>
Subject: Job 110455459: </sc/orga/projects/va-biobank/Veera/genie/modules/predixcan/novel/temp/.snakemake/tmp.3u4xkz3n/all2.clump_two.76> in cluster <minerva> Exited

Job </sc/orga/projects/va-biobank/Veera/genie/modules/predixcan/novel/temp/.snakemake/tmp.3u4xkz3n/all2.clump_two.76> was submitted from host <node26-55> by user <xrajagv01> in cluster <minerva>.
Job was executed on host(s) <node26-50>, in queue <premium>, as user <xrajagv01> in cluster <minerva>.
</hpc/users/xrajagv01> was used as the home directory.
</hpc/users/xrajagv01/va-biobank/Veera/genie/modules/predixcan/novel/temp> was used as the working directory.
Started at Thu Sep  6 05:54:19 2018
Results reported on Thu Sep  6 05:55:26 2018

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/sc/orga/projects/va-biobank/Veera/genie/modules/predixcan/novel/temp/.snakemake/tmp.3u4xkz3n/all2.clump_two.76
------------------------------------------------------------

TERM_MEMLIMIT: job killed after reaching LSF memory usage limit.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   39.44 sec.
    Max Memory :                                 2000 MB
    Average Memory :                             778.82 MB
    Total Requested Memory :                     2000.00 MB
    Delta Memory :                               0.00 MB
    Max Processes :                              9
    Max Threads :                                11

The output (if any) follows:

[Thu Sep  6 05:54:23 2018] Building DAG of jobs...
[Thu Sep  6 05:54:34 2018] Using shell: /bin/bash
[Thu Sep  6 05:54:34 2018] Provided cores: 1
[Thu Sep  6 05:54:34 2018] Rules claiming more threads will be scaled down.
[Thu Sep  6 05:54:34 2018] Job counts:
[Thu Sep  6 05:54:34 2018] 	count	jobs
[Thu Sep  6 05:54:34 2018] 	1	clump_two
[Thu Sep  6 05:54:34 2018] 	1

[Thu Sep  6 05:54:34 2018] rule clump_two:
[Thu Sep  6 05:54:34 2018]     input: genie_clump/all2/all2ld_from_1kg.eur.biallele.snps.maf0.01/CD.gwas.sumstats.clumped, /sc/orga/projects/va-biobank/Veera/downloads/CMC/modules/wen_gwas_files/CD.gwas.sumstats
[Thu Sep  6 05:54:34 2018]     output: genie_clump/all2/all2ld_from_1kg.eur.biallele.snps.maf0.01/CD.gwas.sumstats.clumped.formatted
[Thu Sep  6 05:54:34 2018]     jobid: 0
[Thu Sep  6 05:54:34 2018]     wildcards: wcard=CD.gwas.sumstats

Read 2.6% of 11375862 rowsRead 5.7% of 11375862 rowsRead 9.6% of 11375862 rowsRead 13.0% of 11375862 rowsRead 16.5% of 11375862 rowsRead 20.7% of 11375862 rowsRead 25.7% of 11375862 rowsRead 31.6% of 11375862 rowsRead 36.5% of 11375862 rowsRead 38.9% of 11375862 rowsRead 47.4% of 11375862 rowsRead 47.5% of 11375862 rowsRead 56.1% of 11375862 rowsRead 57.8% of 11375862 rowsRead 66.3% of 11375862 rowsRead 70.2% of 11375862 rowsRead 71.9% of 11375862 rowsRead 81.0% of 11375862 rowsRead 85.1% of 11375862 rowsRead 93.8% of 11375862 rowsRead 11375862 rows and 7 (of 7) columns from 0.482 GB file in 00:00:33
/bin/bash: line 1: 26165 Killed                  Rscript /sc/orga/projects/va-biobank/Veera/genie/modules/sumstats/clump/add.alleles.R /sc/orga/projects/va-biobank/Veera/downloads/CMC/modules/wen_gwas_files/CD.gwas.sumstats genie_clump/all2/all2ld_from_1kg.eur.biallele.snps.maf0.01/CD.gwas.sumstats.clumped genie_clump/all2/all2ld_from_1kg.eur.biallele.snps.maf0.01/CD.gwas.sumstats.clumped.formatted
[Thu Sep  6 05:55:25 2018]     Error in rule clump_two:
[Thu Sep  6 05:55:25 2018]         jobid: 0
[Thu Sep  6 05:55:25 2018]         output: genie_clump/all2/all2ld_from_1kg.eur.biallele.snps.maf0.01/CD.gwas.sumstats.clumped.formatted

[Thu Sep  6 05:55:25 2018] RuleException:
[Thu Sep  6 05:55:25 2018] CalledProcessError in line 44 of /sc/orga/projects/va-biobank/Veera/genie/modules/sumstats/clump/clump.snake:
[Thu Sep  6 05:55:25 2018] Command ' set -euo pipefail;  Rscript /sc/orga/projects/va-biobank/Veera/genie/modules/sumstats/clump/add.alleles.R /sc/orga/projects/va-biobank/Veera/downloads/CMC/modules/wen_gwas_files/CD.gwas.sumstats genie_clump/all2/all2ld_from_1kg.eur.biallele.snps.maf0.01/CD.gwas.sumstats.clumped genie_clump/all2/all2ld_from_1kg.eur.biallele.snps.maf0.01/CD.gwas.sumstats.clumped.formatted ' returned non-zero exit status 137.
[Thu Sep  6 05:55:25 2018]   File "/sc/orga/projects/va-biobank/Veera/genie/modules/sumstats/clump/clump.snake", line 44, in __rule_clump_two
[Thu Sep  6 05:55:25 2018]   File "/hpc/users/xrajagv01/.conda/envs/genie/lib/python3.6/concurrent/futures/thread.py", line 56, in run
[Thu Sep  6 05:55:25 2018] Shutting down, this might take some time.
[Thu Sep  6 05:55:25 2018] Exiting because a job execution failed. Look above for error message
